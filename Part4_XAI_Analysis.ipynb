{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pvXBcx0KT1d"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# PART 4: EXPLAINABLE AI (FAST VERSION)\n",
        "# AI Drone Anomaly Detection\n",
        "# ==============================\n",
        "\n",
        "# Install packages if running in Colab\n",
        "!pip install --quiet shap lime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# ------------------------------\n",
        "# Step 1: Load preprocessed dataset\n",
        "# ------------------------------\n",
        "data = pd.read_csv('/content/sample_data/drone_data_preprocessed.csv')\n",
        "X = data.drop('Label', axis=1).values\n",
        "y = data['Label'].values\n",
        "feature_names = data.drop('Label', axis=1).columns\n",
        "\n",
        "# Train-test split (match fast version subset)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, train_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 2: Load models\n",
        "# ------------------------------\n",
        "xgb_model = joblib.load(\"xgb_model_fast.pkl\")\n",
        "fnn_model = load_model(\"fnn_model_fast.h5\")\n",
        "\n",
        "# ------------------------------\n",
        "# Step 3: XGBoost Feature Importance\n",
        "# ------------------------------\n",
        "xgb_importance = xgb_model.best_estimator_.feature_importances_\n",
        "xgb_feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': xgb_importance}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Importance', y='Feature', data=xgb_feat_df.head(15))\n",
        "plt.title(\"Top 15 XGBoost Feature Importances\")\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------\n",
        "# Step 4: SHAP Analysis\n",
        "# ------------------------------\n",
        "# --- 4a: XGBoost SHAP ---\n",
        "explainer_xgb = shap.TreeExplainer(xgb_model.best_estimator_)\n",
        "shap_values_xgb_all = explainer_xgb.shap_values(X_test)\n",
        "\n",
        "class_index = 1\n",
        "shap_values_xgb = shap_values_xgb_all[class_index]\n",
        "shap.summary_plot(shap_values_xgb, X_test, feature_names=feature_names, plot_type='bar')\n",
        "\n",
        "# SHAP dependence plot for top 3 features\n",
        "top_features = xgb_feat_df['Feature'].head(3).tolist()\n",
        "for feat in top_features:\n",
        "    shap.dependence_plot(feat,shap_values_xgb,X_test,feature_names=feature_names)\n",
        "\n",
        "# --- 4b: FNN SHAP ---\n",
        "# Use small background for speed\n",
        "background = X_train_scaled[np.random.choice(X_train_scaled.shape[0], 50, replace=False)]\n",
        "\n",
        "def fnn_predict(X):\n",
        "    return fnn_model.predict(X)\n",
        "\n",
        "explainer_fnn = shap.KernelExplainer(fnn_predict, background)\n",
        "shap_values_fnn = explainer_fnn.shap_values(X_test_scaled[:50], nsamples=100)\n",
        "\n",
        "shap.summary_plot(shap_values_fnn, X_test_scaled[:50], feature_names=feature_names, plot_type='bar')\n",
        "\n",
        "# ------------------------------\n",
        "# Step 5: LIME Explanations (FNN)\n",
        "# ------------------------------\n",
        "explainer_lime = LimeTabularExplainer(\n",
        "    X_train_scaled,\n",
        "    feature_names=feature_names,\n",
        "    class_names=[str(c) for c in np.unique(y_train)],\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "# Explain 3 sample predictions\n",
        "for i in range(3):\n",
        "    exp = explainer_lime.explain_instance(X_test_scaled[i], fnn_model.predict)\n",
        "    print(f\"LIME explanation for sample {i}\")\n",
        "    exp.show_in_notebook(show_table=True)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 6: Partial Dependence Plots (Top Features)\n",
        "# ------------------------------\n",
        "top_features = xgb_feat_df['Feature'].head(5).tolist()\n",
        "PartialDependenceDisplay.from_estimator(\n",
        "    xgb_model.best_estimator_,\n",
        "    X_test,\n",
        "    features=top_features,\n",
        "    feature_names=feature_names,\n",
        "    grid_resolution=20\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "print(\"FAST VERSION: XAI analysis complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "35flArc7N9eH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}